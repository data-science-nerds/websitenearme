{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "50dvxjqCFmhF"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load environment variables\n",
        "# python -m ipykernel install --user --name=webnearme-venv\n",
        "from dotenv import load_dotenv,find_dotenv\n",
        "load_dotenv(find_dotenv())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "C7RnyUOCJWmk"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n\\nLarge language models are a type of artificial intelligence (AI) that can generate human-like text by training on large amounts of text data.'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Run basic query with OpenAI wrapper\n",
        "\n",
        "from langchain.llms import OpenAI\n",
        "llm = OpenAI(model_name=\"text-davinci-003\")\n",
        "llm(\"explain large language models in one sentence\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JtHgQ5XpJmgi"
      },
      "outputs": [],
      "source": [
        "# import schema for chat messages and ChatOpenAI in order to query chatmodels GPT-3.5-turbo or GPT-4\n",
        "\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "from langchain.chat_models import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yrfYfKfdJyyF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sure! Here's an example script that trains a neural network on simulated data using the Keras library:\n",
            "\n",
            "```python\n",
            "import numpy as np\n",
            "from keras.models import Sequential\n",
            "from keras.layers import Dense\n",
            "\n",
            "# Generate simulated data\n",
            "np.random.seed(0)\n",
            "X = np.random.rand(100, 2)\n",
            "y = np.random.randint(0, 2, size=(100,))\n",
            "\n",
            "# Define the neural network model\n",
            "model = Sequential()\n",
            "model.add(Dense(10, input_dim=2, activation='relu'))\n",
            "model.add(Dense(1, activation='sigmoid'))\n",
            "\n",
            "# Compile the model\n",
            "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
            "\n",
            "# Train the model\n",
            "model.fit(X, y, epochs=10, batch_size=10)\n",
            "\n",
            "# Evaluate the model\n",
            "loss, accuracy = model.evaluate(X, y)\n",
            "print(f\"Loss: {loss}, Accuracy: {accuracy}\")\n",
            "```\n",
            "\n",
            "In this script, we first generate simulated data using `numpy.random.rand()` and `numpy.random.randint()`. We then define a simple neural network model using the `Sequential` class from Keras. The model consists of two dense layers with 10 and 1 units respectively, and uses the ReLU and sigmoid activation functions.\n",
            "\n",
            "Next, we compile the model using the binary cross-entropy loss function and the Adam optimizer. We then train the model using the `fit()` method, specifying the number of epochs and batch size.\n",
            "\n",
            "Finally, we evaluate the trained model on the same data and print the loss and accuracy.\n"
          ]
        }
      ],
      "source": [
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\",temperature=0.3)\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are an expert data scientist\"),\n",
        "    HumanMessage(content=\"Write a Python script that trains a neural network on simulated data \")\n",
        "]\n",
        "response=chat(messages)\n",
        "\n",
        "print(response.content,end='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2grf7I8AJ_hK"
      },
      "outputs": [],
      "source": [
        "# Import prompt and define PromptTemplate\n",
        "\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "You are an expert data scientist with an expertise in building deep learning models. \n",
        "Explain the concept of {concept} in a couple of lines\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"concept\"],\n",
        "    template=template,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vcz7Q9Y-KFvI"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nVector is a mathematical object that has magnitude and direction. It is used to represent physical quantities such as force, velocity, and acceleration, as well as abstract quantities such as temperature, pressure, and electric current. In deep learning, vectors are used to represent data points, which can be used as inputs to a model or as outputs from a model.'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Run LLM with PromptTemplate\n",
        "\n",
        "llm(prompt.format(concept=\"vector\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dm78i-rUKXIB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "An autoencoder is a neural network architecture used for unsupervised learning, where the network learns to represent (or encode) an input in a compressed form and then reconstruct it (or decode it) to match the original input. It is typically used for dimensionality reduction and feature extraction.\n"
          ]
        }
      ],
      "source": [
        "# Import LLMChain and define chain with language model and prompt as arguments.\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Run the chain only specifying the input variable.\n",
        "print(chain.run(\"autoencoder\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "B6MF4-nMKul3"
      },
      "outputs": [],
      "source": [
        "# Define a second prompt \n",
        "\n",
        "second_prompt = PromptTemplate(\n",
        "    input_variables=[\"ml_concept\"],\n",
        "    template=\"Turn the concept description of {ml_concept} and explain it to me like I'm five in 500 words\",\n",
        ")\n",
        "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SkJKFyk1K-MO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3m\n",
            "An autoencoder is a type of artificial neural network that learns to represent input data as a compressed representation, or \"embedding\". It is used to learn efficient data encodings in an unsupervised manner by reconstructing the input data from the learned embedding.\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3m\n",
            "\n",
            "An autoencoder is a computer program that can learn to make data more compact and easier to understand. It's like a special kind of puzzle.\n",
            "\n",
            "Imagine you have a big jigsaw puzzle with lots of pieces. To make it easier to put together, you could sort the pieces into piles of different shapes. That would make it easier to find the pieces that go together. \n",
            "\n",
            "An autoencoder works in a similar way. It looks at a set of data and finds ways to sort it into different piles. It tries to make the piles as small as possible, so that it can fit more data into each pile. This way, the autoencoder can make the data more compact and easier to understand.\n",
            "\n",
            "The autoencoder learns how to sort the data by trying to reconstruct the original data from the sorted piles. It can do this by using a type of artificial intelligence called a neural network. This is like a network of computers that can learn and adjust how they work over time.\n",
            "\n",
            "The autoencoder looks at the data and tries to figure out the best way to put it into piles. It then creates a special kind of representation of the data that makes it easier to understand. This representation\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "An autoencoder is a computer program that can learn to make data more compact and easier to understand. It's like a special kind of puzzle.\n",
            "\n",
            "Imagine you have a big jigsaw puzzle with lots of pieces. To make it easier to put together, you could sort the pieces into piles of different shapes. That would make it easier to find the pieces that go together. \n",
            "\n",
            "An autoencoder works in a similar way. It looks at a set of data and finds ways to sort it into different piles. It tries to make the piles as small as possible, so that it can fit more data into each pile. This way, the autoencoder can make the data more compact and easier to understand.\n",
            "\n",
            "The autoencoder learns how to sort the data by trying to reconstruct the original data from the sorted piles. It can do this by using a type of artificial intelligence called a neural network. This is like a network of computers that can learn and adjust how they work over time.\n",
            "\n",
            "The autoencoder looks at the data and tries to figure out the best way to put it into piles. It then creates a special kind of representation of the data that makes it easier to understand. This representation\n"
          ]
        }
      ],
      "source": [
        "# Define a sequential chain using the two chains above: the second chain takes the output of the first chain as input\n",
        "\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "overall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\n",
        "\n",
        "# Run the chain specifying only the input variable for the first chain.\n",
        "explanation = overall_chain.run(\"autoencoder\")\n",
        "print(explanation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "mDDu1B_SLQls"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import utility for splitting up texts and split up the explanation given above into document chunks\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 100,\n",
        "    chunk_overlap  = 0,\n",
        ")\n",
        "\n",
        "texts = text_splitter.create_documents([explanation])\n",
        "len(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "F6lfAdeuLhtp"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'An autoencoder is a computer program that can learn to make data more compact and easier to'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Individual text chunks can be accessed with \"page_content\"\n",
        "\n",
        "texts[0].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Z5sv4e3tLw2y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'langchain.embeddings.openai.OpenAIEmbeddings'>\n"
          ]
        }
      ],
      "source": [
        "# Import and instantiate OpenAI embeddings\n",
        "\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model_name=\"ada\")\n",
        "print(type(embeddings))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dqzoir4hMlfl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-0.028041064536572253, 0.03055861942736854, 0.014163807863452081]\n"
          ]
        }
      ],
      "source": [
        "# Turn the first text chunk into a vector with the embedding\n",
        "\n",
        "query_result = embeddings.embed_query(texts[0].page_content)\n",
        "print(query_result[0:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "QaOY5bIZM3Xz"
      },
      "outputs": [],
      "source": [
        "# Import and initialize Pinecone client\n",
        "\n",
        "import os\n",
        "import pinecone\n",
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "\n",
        "pinecone.init(\n",
        "    api_key=os.getenv('PINECONE_API_KEY'),  \n",
        "    environment=os.getenv('PINECONE_ENV')  \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "lZhSUt3FNBzN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "page_content='An autoencoder is a computer program that can learn to make data more compact and easier to' metadata={'text': 'An autoencoder is a computer program that can learn to make data more compact and easier to'}\n"
          ]
        }
      ],
      "source": [
        "# # Upload vectors to Pinecone\n",
        "# # import pinecone\n",
        "# import pinecone      \n",
        "# # pinecone.list_indexes()\n",
        "\n",
        "# pinecone.create_index('websitenearme-fast-api', dimension=1024)\n",
        "# index_name = pinecone.index.Index('websitenearme-fast-api')\n",
        "# # print(index_name)\n",
        "\n",
        "# # index_name = \"websitenearme-fast-api\"\n",
        "# search = pinecone.from_existing_index(texts=texts, embeddings=embeddings, index_name=index_name)\n",
        "print(texts[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from requests.packages.urllib3.util.ssl_ import create_urllib3_context\n",
        "\n",
        "CIPHERS = (\n",
        "    'ECDHE+AESGCM:ECDHE+CHACHA20:DHE+AESGCM:DHE+CHACHA20:ECDH+AESGCM:ECDH+CHACHA20:DH+AESGCM:DH+CHACHA20:'\n",
        "    'ECDHE+AES:!aNULL:!eNULL:!EXPORT:!DES:!MD5:!PSK:!RC4:!HMAC_SHA1:!SHA1:!DHE+AES:!ECDH+AES:!DH+AES'\n",
        ")\n",
        "\n",
        "requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS = CIPHERS\n",
        "# Skip the following two lines if they cause errors\n",
        "# requests.packages.urllib3.contrib.pyopenssl.DEFAULT_SSL_CIPHER_LIST = CIPHERS\n",
        "# requests.packages.urllib3.contrib.pyopenssl.inject_into_urllib3()\n",
        "requests.packages.urllib3.util.ssl_.create_default_context = create_urllib3_context\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<langchain.vectorstores.pinecone.Pinecone object at 0x12a0798d0>\n"
          ]
        }
      ],
      "source": [
        "# Initialize Pinecone\n",
        "index_name = \"websitenearme-fast-api\"\n",
        "DIMENSIONS=128\n",
        "\n",
        "# Create and configure index if doesn't already exist\n",
        "if index_name not in pinecone.list_indexes():\n",
        "    pinecone.create_index(\n",
        "        name=index_name, \n",
        "        metric=\"cosine\",\n",
        "        dimension=DIMENSIONS)\n",
        "    docsearch = Pinecone.from_documents(texts[0], embeddings, index_name=index_name)\n",
        "\n",
        "else:\n",
        "    docsearch = Pinecone.from_existing_index(index_name, embeddings)\n",
        "\n",
        "print(docsearch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['websitenearme-fast-api']"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pinecone      \n",
        "pinecone.list_indexes()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "IndexDescription(name='websitenearme-fast-api', metric='cosine', replicas=1, dimension=128.0, shards=1, pods=1, pod_type='p1', status={'ready': True, 'state': 'Ready'}, metadata_config=None, source_collection='')"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pinecone.describe_index(index_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## we created the index up to this point\n",
        "## next we will insert the data into pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "cCXVuXwPNKcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "# Do a simple vector similarity search\n",
        "\n",
        "query = \"What is magical about an autoencoder?\"\n",
        "result = docsearch.similarity_search(query)\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ogz8luZNRnJ"
      },
      "outputs": [],
      "source": [
        "# Import Python REPL tool and instantiate Python agent\n",
        "\n",
        "from langchain.agents.agent_toolkits import create_python_agent\n",
        "from langchain.tools.python.tool import PythonREPLTool\n",
        "from langchain.python import PythonREPL\n",
        "from langchain.llms.openai import OpenAI\n",
        "\n",
        "agent_executor = create_python_agent(\n",
        "    llm=OpenAI(temperature=0, max_tokens=1000),\n",
        "    tool=PythonREPLTool(),\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVHMDj0sNi09"
      },
      "outputs": [],
      "source": [
        "# Execute the Python agent\n",
        "\n",
        "agent_executor.run(\"Find the roots (zeros) if the quadratic function 3 * x**2 + 2*x -1\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyONM96f7/m0jUCD9c87+MQy",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
